services:
  ollama:
    image: ollama/ollama:latest
    container_name: eventify-ollama
    restart: unless-stopped
    # Only start when LLM features are needed
    profiles:
      - llm
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - monorepo-network
    environment:
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
    # Resource limits to prevent memory issues
    deploy:
      resources:
        limits:
          memory: 1.5G
          cpus: "1.5"

  api:
    container_name: eventify-api-dev
    restart: unless-stopped
    build:
      context: .
      target: development
    ports:
      - 4001:3007
    volumes:
      - ./:/usr/src/eventify
      - /usr/src/eventify/node_modules
      - ./uploads:/usr/src/eventify/uploads
    working_dir: /usr/src/eventify
    env_file:
      - .env.dev
    # Resource limits for development
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"
    networks:
      - monorepo-network

    # FOR DEVELOPMENT
    command: sh -c "npm install && npm run dev"

  batch:
    container_name: eventify-batch-dev
    restart: unless-stopped
    build:
      context: .
      target: development
    ports:
      - 4002:3008
    volumes:
      - ./:/usr/src/eventify
      - /usr/src/eventify/node_modules
      - ./uploads:/usr/src/eventify/uploads
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /usr/src/eventify
    env_file:
      - .env.dev
    # Resource limits for development
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"
    networks:
      - monorepo-network
    command: sh -c "apk add --no-cache docker-cli && while [ ! -d node_modules ]; do sleep 2; done && npm run dev:batch"

volumes:
  ollama_data:

networks:
  monorepo-network:
    driver: bridge
