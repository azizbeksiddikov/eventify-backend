services:
  ollama:
    image: ollama/ollama:latest
    container_name: eventify-ollama
    restart: unless-stopped
    # Only start when LLM features are needed
    profiles:
      - llm
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - monorepo-network
    environment:
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
    # Resource limits to prevent memory issues
    deploy:
      resources:
        limits:
          memory: 1000M  # 1GB for AI/Ollama
          cpus: "0.8"

  api:
    container_name: eventify-api-dev
    image: eventify-api:dev
    restart: unless-stopped
    build:
      context: .
      target: development
    ports:
      - 4001:3007
    volumes:
      - ./apps:/usr/src/eventify/apps
      - ./uploads:/usr/src/eventify/uploads
    working_dir: /usr/src/eventify
    env_file:
      - .env.dev
    # Resource limits for development
    deploy:
      resources:
        limits:
          memory: 800M  # 800 MB for API
          cpus: "1.0"
    networks:
      - monorepo-network

    command: npm run dev

  batch:
    container_name: eventify-batch-dev
    image: eventify-batch:dev
    restart: unless-stopped
    build:
      context: .
      target: development
    ports:
      - 4002:3008
    volumes:
      - ./apps:/usr/src/eventify/apps
      - ./uploads:/usr/src/eventify/uploads
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /usr/src/eventify
    env_file:
      - .env.dev
    # Resource limits for development
    deploy:
      resources:
        limits:
          memory: 800M  # 800 MB for Batch
          cpus: "1.0"
    networks:
      - monorepo-network
    # Docker CLI already in base image
    command: npm run dev:batch

volumes:
  ollama_data:

networks:
  monorepo-network:
    driver: bridge
