services:
  ollama:
    image: ollama/ollama:latest
    container_name: eventify-ollama
    restart: unless-stopped
    # Only start when LLM features are needed
    profiles:
      - llm
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - monorepo-network-dev
    environment:
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
    # Resource limits to prevent memory issues
    deploy:
      resources:
        limits:
          memory: 1500M
          cpus: "1.0"

  api:
    container_name: eventify-api-dev
    image: eventify-api:dev
    restart: unless-stopped
    build:
      context: .
      target: development
    ports:
      - 5001:3007
    volumes:
      - ./apps:/usr/src/eventify/apps
      - ./uploads:/usr/src/eventify/uploads
      - /var/run/docker.sock:/var/run/docker.sock # Allow Docker control
    working_dir: /usr/src/eventify
    env_file:
      - .env.dev
    # Resource limits for development
    deploy:
      resources:
        limits:
          memory: 800M # 800 MB for API
          cpus: "1.0"
    networks:
      - monorepo-network-dev

    command: npm run dev

  batch:
    container_name: eventify-batch-dev
    image: eventify-batch:dev
    restart: unless-stopped
    build:
      context: .
      target: development
    ports:
      - 5002:3008
    volumes:
      - ./apps:/usr/src/eventify/apps
      - ./uploads:/usr/src/eventify/uploads
      - ./jsons:/usr/src/eventify/jsons
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /usr/src/eventify
    env_file:
      - .env.dev
    environment:
      - SAVE_JSON_FILES=true
    # Resource limits for development
    deploy:
      resources:
        limits:
          memory: 800M # 800 MB for Batch
          cpus: "1.0"
    networks:
      - monorepo-network-dev
    # Docker CLI already in base image
    command: npm run dev:batch

volumes:
  ollama_data:
    name: ollama_data

networks:
  monorepo-network-dev:
    name: monorepo-network-dev
    driver: bridge
