services:
  ollama:
    image: ollama/ollama:latest
    container_name: eventify-ollama
    restart: unless-stopped
    profiles:
      - llm
    ports:
      - "11434:11434"
    volumes:
      - ollama_data_prod:/root/.ollama
    networks:
      - monorepo-network-prod
    environment:
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
    # Resource limits to prevent memory issues
    deploy:
      resources:
        limits:
          memory: 1000M # 1GB for AI/Ollama
          cpus: "0.8"

  api:
    container_name: eventify-api-prod
    image: eventify-api:prod
    restart: unless-stopped
    build:
      context: .
      target: production
    ports:
      - 4001:3007
    volumes:
      - ./uploads:/usr/src/eventify/uploads
    working_dir: /usr/src/eventify
    env_file:
      - .env
    deploy:
      resources:
        limits:
          memory: 800M # 800 MB for API
          cpus: "1.0"
    networks:
      - monorepo-network-prod
    command: node dist/apps/api/main

  batch:
    container_name: eventify-batch-prod
    image: eventify-batch:prod
    restart: unless-stopped
    build:
      context: .
      target: production
    ports:
      - 4002:3008
    volumes:
      - ./uploads:/usr/src/eventify/uploads
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /usr/src/eventify
    env_file:
      - .env
    environment:
      - SAVE_JSON_FILES=false
    deploy:
      resources:
        limits:
          memory: 800M # 800 MB for Batch
          cpus: "0.5"
    networks:
      - monorepo-network-prod
    command: node dist/apps/batch/main

volumes:
  ollama_data_prod:
    name: ollama_data_prod

networks:
  monorepo-network-prod:
    name: monorepo-network-prod
    driver: bridge
